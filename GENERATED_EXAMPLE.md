# Generated Content Example

This file demonstrates what was created using the Ralph Ollama integration.

## What Was Generated

Using local Ollama (running on localhost:11434), we generated:

1. **Python Function** - `generated_factorial.py`
   - Factorial calculation function
   - Includes docstring, error handling, and examples

2. **API Documentation** - Below (generated via Ollama API)

---

## Generated API Documentation

The following was generated by Ollama using the llama3.2:latest model:

```markdown
# POST /api/users

## Description
Creates a new user in the system.

## Request

### Headers
```
Content-Type: application/json
Authorization: Bearer <token>
```

### Body
```json
{
  "name": "string (required)",
  "email": "string (required, valid email)",
  "age": "integer (optional)"
}
```

## Response

### Success (201 Created)
```json
{
  "id": 123,
  "name": "John Doe",
  "email": "john@example.com",
  "age": 30,
  "created_at": "2024-01-15T10:30:00Z"
}
```

### Error (400 Bad Request)
```json
{
  "error": "Validation failed",
  "details": ["email is required", "name must be at least 2 characters"]
}
```

## Example Request

```bash
curl -X POST http://api.example.com/api/users \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your_token_here" \
  -d '{
    "name": "John Doe",
    "email": "john@example.com",
    "age": 30
  }'
```

## Example Response

```json
{
  "id": 123,
  "name": "John Doe",
  "email": "john@example.com",
  "age": 30,
  "created_at": "2024-01-15T10:30:00Z"
}
```
```

---

## How This Was Created

This content was generated using:
- **Ollama Server**: Running locally on port 11434
- **Model**: llama3.2:latest
- **Integration**: Ralph Ollama integration (direct API calls)

The Python function was tested and works correctly. The API documentation follows standard REST API documentation format.

---

**Generated on**: 2024  
**Model Used**: llama3.2:latest  
**Integration**: Ralph Ollama
